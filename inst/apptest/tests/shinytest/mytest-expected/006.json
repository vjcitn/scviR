{
  "input": {
    "btnSend": 0,
    "curmod": "scvi",
    "lev2": "TOTALVAE",
    "topmods": "module"
  },
  "output": {
    "pickedmodule": {
      "html": "<div id=\"lev2\" class=\"form-group shiny-input-radiogroup shiny-input-container shiny-input-container-inline\" role=\"radiogroup\" aria-labelledby=\"lev2-label\">\n  <label class=\"control-label\" id=\"lev2-label\" for=\"lev2\">subtop<\/label>\n  <div class=\"shiny-options-group\">\n    <label class=\"radio-inline\">\n      <input type=\"radio\" name=\"lev2\" value=\"AmortizedLDAPyroModule\" checked=\"checked\"/>\n      <span>AmortizedLDAPyroModule<\/span>\n    <\/label>\n    <label class=\"radio-inline\">\n      <input type=\"radio\" name=\"lev2\" value=\"AutoZIVAE\"/>\n      <span>AutoZIVAE<\/span>\n    <\/label>\n    <label class=\"radio-inline\">\n      <input type=\"radio\" name=\"lev2\" value=\"base\"/>\n      <span>base<\/span>\n    <\/label>\n    <label class=\"radio-inline\">\n      <input type=\"radio\" name=\"lev2\" value=\"Classifier\"/>\n      <span>Classifier<\/span>\n    <\/label>\n    <label class=\"radio-inline\">\n      <input type=\"radio\" name=\"lev2\" value=\"JaxVAE\"/>\n      <span>JaxVAE<\/span>\n    <\/label>\n    <label class=\"radio-inline\">\n      <input type=\"radio\" name=\"lev2\" value=\"LDVAE\"/>\n      <span>LDVAE<\/span>\n    <\/label>\n    <label class=\"radio-inline\">\n      <input type=\"radio\" name=\"lev2\" value=\"MRDeconv\"/>\n      <span>MRDeconv<\/span>\n    <\/label>\n    <label class=\"radio-inline\">\n      <input type=\"radio\" name=\"lev2\" value=\"MULTIVAE\"/>\n      <span>MULTIVAE<\/span>\n    <\/label>\n    <label class=\"radio-inline\">\n      <input type=\"radio\" name=\"lev2\" value=\"PEAKVAE\"/>\n      <span>PEAKVAE<\/span>\n    <\/label>\n    <label class=\"radio-inline\">\n      <input type=\"radio\" name=\"lev2\" value=\"SCANVAE\"/>\n      <span>SCANVAE<\/span>\n    <\/label>\n    <label class=\"radio-inline\">\n      <input type=\"radio\" name=\"lev2\" value=\"TOTALVAE\"/>\n      <span>TOTALVAE<\/span>\n    <\/label>\n    <label class=\"radio-inline\">\n      <input type=\"radio\" name=\"lev2\" value=\"VAE\"/>\n      <span>VAE<\/span>\n    <\/label>\n    <label class=\"radio-inline\">\n      <input type=\"radio\" name=\"lev2\" value=\"VAEC\"/>\n      <span>VAEC<\/span>\n    <\/label>\n  <\/div>\n<\/div>",
      "deps": [

      ]
    },
    "subtext": "Help on class TOTALVAE in module scvi.module._totalvae:<br><br>class TOTALVAE(scvi.module.base._base_module.BaseModuleClass)<br> |  TOTALVAE(n_input_genes: int, n_input_proteins: int, n_batch: int = 0, n_labels: int = 0, n_hidden: scvi.autotune._types.Tunable_ = 256, n_latent: scvi.autotune._types.Tunable_ = 20, n_layers_encoder: scvi.autotune._types.Tunable_ = 2, n_layers_decoder: scvi.autotune._types.Tunable_ = 1, n_continuous_cov: int = 0, n_cats_per_cov: Union[Iterable[int], NoneType] = None, dropout_rate_decoder: scvi.autotune._types.Tunable_ = 0.2, dropout_rate_encoder: scvi.autotune._types.Tunable_ = 0.2, gene_dispersion: scvi.autotune._types.Tunable_ = 'gene', protein_dispersion: scvi.autotune._types.Tunable_ = 'protein', log_variational: bool = True, gene_likelihood: scvi.autotune._types.Tunable_ = 'nb', latent_distribution: scvi.autotune._types.Tunable_ = 'normal', protein_batch_mask: Dict[Union[str, int], numpy.ndarray] = None, encode_covariates: bool = True, protein_background_prior_mean: Union[numpy.ndarray, NoneType] = None, protein_background_prior_scale: Union[numpy.ndarray, NoneType] = None, use_size_factor_key: bool = False, use_observed_lib_size: bool = True, library_log_means: Union[numpy.ndarray, NoneType] = None, library_log_vars: Union[numpy.ndarray, NoneType] = None, use_batch_norm: scvi.autotune._types.Tunable_ = 'both', use_layer_norm: scvi.autotune._types.Tunable_ = 'none')<br> |  <br> |  Total variational inference for CITE-seq data.<br> |  <br> |  Implements the totalVI model of :cite:p:`GayosoSteier21`.<br> |  <br> |  Parameters<br> |  ----------<br> |  n_input_genes<br> |      Number of input genes<br> |  n_input_proteins<br> |      Number of input proteins<br> |  n_batch<br> |      Number of batches<br> |  n_labels<br> |      Number of labels<br> |  n_hidden<br> |      Number of nodes per hidden layer for encoder and decoder<br> |  n_latent<br> |      Dimensionality of the latent space<br> |  n_layers<br> |      Number of hidden layers used for encoder and decoder NNs<br> |  n_continuous_cov<br> |      Number of continuous covarites<br> |  n_cats_per_cov<br> |      Number of categories for each extra categorical covariate<br> |  dropout_rate<br> |      Dropout rate for neural networks<br> |  gene_dispersion<br> |      One of the following<br> |  <br> |      * ``'gene'`` - genes_dispersion parameter of NB is constant per gene across cells<br> |      * ``'gene-batch'`` - genes_dispersion can differ between different batches<br> |      * ``'gene-label'`` - genes_dispersion can differ between different labels<br> |  protein_dispersion<br> |      One of the following<br> |  <br> |      * ``'protein'`` - protein_dispersion parameter is constant per protein across cells<br> |      * ``'protein-batch'`` - protein_dispersion can differ between different batches NOT TESTED<br> |      * ``'protein-label'`` - protein_dispersion can differ between different labels NOT TESTED<br> |  log_variational<br> |      Log(data+1) prior to encoding for numerical stability. Not normalization.<br> |  gene_likelihood<br> |      One of<br> |  <br> |      * ``'nb'`` - Negative binomial distribution<br> |      * ``'zinb'`` - Zero-inflated negative binomial distribution<br> |  latent_distribution<br> |      One of<br> |  <br> |      * ``'normal'`` - Isotropic normal<br> |      * ``'ln'`` - Logistic normal with normal params N(0, 1)<br> |  protein_batch_mask<br> |      Dictionary where each key is a batch code, and value is for each protein, whether it was observed or not.<br> |  encode_covariates<br> |      Whether to concatenate covariates to expression in encoder<br> |  protein_background_prior_mean<br> |      Array of proteins by batches, the prior initialization for the protein background mean (log scale)<br> |  protein_background_prior_scale<br> |      Array of proteins by batches, the prior initialization for the protein background scale (log scale)<br> |  use_size_factor_key<br> |      Use size_factor AnnDataField defined by the user as scaling factor in mean of conditional distribution.<br> |      Takes priority over `use_observed_lib_size`.<br> |  use_observed_lib_size<br> |      Use observed library size for RNA as scaling factor in mean of conditional distribution<br> |  library_log_means<br> |      1 x n_batch array of means of the log library sizes. Parameterizes prior on library size if<br> |      not using observed library size.<br> |  library_log_vars<br> |      1 x n_batch array of variances of the log library sizes. Parameterizes prior on library size if<br> |      not using observed library size.<br> |  <br> |  Method resolution order:<br> |      TOTALVAE<br> |      scvi.module.base._base_module.BaseModuleClass<br> |      scvi.autotune._types.TunableMixin<br> |      torch.nn.modules.module.Module<br> |      builtins.object<br> |  <br> |  Methods defined here:<br> |  <br> |  __init__(self, n_input_genes: int, n_input_proteins: int, n_batch: int = 0, n_labels: int = 0, n_hidden: scvi.autotune._types.Tunable_ = 256, n_latent: scvi.autotune._types.Tunable_ = 20, n_layers_encoder: scvi.autotune._types.Tunable_ = 2, n_layers_decoder: scvi.autotune._types.Tunable_ = 1, n_continuous_cov: int = 0, n_cats_per_cov: Union[Iterable[int], NoneType] = None, dropout_rate_decoder: scvi.autotune._types.Tunable_ = 0.2, dropout_rate_encoder: scvi.autotune._types.Tunable_ = 0.2, gene_dispersion: scvi.autotune._types.Tunable_ = 'gene', protein_dispersion: scvi.autotune._types.Tunable_ = 'protein', log_variational: bool = True, gene_likelihood: scvi.autotune._types.Tunable_ = 'nb', latent_distribution: scvi.autotune._types.Tunable_ = 'normal', protein_batch_mask: Dict[Union[str, int], numpy.ndarray] = None, encode_covariates: bool = True, protein_background_prior_mean: Union[numpy.ndarray, NoneType] = None, protein_background_prior_scale: Union[numpy.ndarray, NoneType] = None, use_size_factor_key: bool = False, use_observed_lib_size: bool = True, library_log_means: Union[numpy.ndarray, NoneType] = None, library_log_vars: Union[numpy.ndarray, NoneType] = None, use_batch_norm: scvi.autotune._types.Tunable_ = 'both', use_layer_norm: scvi.autotune._types.Tunable_ = 'none')<br> |      Initialize self.  See help(type(self)) for accurate signature.<br> |  <br> |  generative(self, z: torch.Tensor, library_gene: torch.Tensor, batch_index: torch.Tensor, label: torch.Tensor, cont_covs=None, cat_covs=None, size_factor=None, transform_batch: Union[int, NoneType] = None) -> Dict[str, Union[torch.Tensor, Dict[str, torch.Tensor]]]<br> |      Run the generative step.<br> |  <br> |  get_reconstruction_loss(self, x: torch.Tensor, y: torch.Tensor, px_dict: Dict[str, torch.Tensor], py_dict: Dict[str, torch.Tensor], pro_batch_mask_minibatch: Union[torch.Tensor, NoneType] = None) -> Tuple[torch.Tensor, torch.Tensor]<br> |      Compute reconstruction loss.<br> |  <br> |  get_sample_dispersion(self, x: torch.Tensor, y: torch.Tensor, batch_index: Union[torch.Tensor, NoneType] = None, label: Union[torch.Tensor, NoneType] = None, n_samples: int = 1) -> Tuple[torch.Tensor, torch.Tensor]<br> |      Returns the tensors of dispersions for genes and proteins.<br> |      <br> |      Parameters<br> |      ----------<br> |      x<br> |          tensor of values with shape ``(batch_size, n_input_genes)``<br> |      y<br> |          tensor of values with shape ``(batch_size, n_input_proteins)``<br> |      batch_index<br> |          array that indicates which batch the cells belong to with shape ``batch_size``<br> |      label<br> |          tensor of cell-types labels with shape ``(batch_size, n_labels)``<br> |      n_samples<br> |          number of samples<br> |      <br> |      Returns<br> |      -------<br> |      type<br> |          tensors of dispersions of the negative binomial distribution<br> |  <br> |  inference(self, x: torch.Tensor, y: torch.Tensor, batch_index: Union[torch.Tensor, NoneType] = None, label: Union[torch.Tensor, NoneType] = None, n_samples=1, cont_covs=None, cat_covs=None) -> Dict[str, Union[torch.Tensor, Dict[str, torch.Tensor]]]<br> |      Internal helper function to compute necessary inference quantities.<br> |      <br> |      We use the dictionary ``px_`` to contain the parameters of the ZINB/NB for genes.<br> |      The rate refers to the mean of the NB, dropout refers to Bernoulli mixing parameters.<br> |      `scale` refers to the quanity upon which differential expression is performed. For genes,<br> |      this can be viewed as the mean of the underlying gamma distribution.<br> |      <br> |      We use the dictionary ``py_`` to contain the parameters of the Mixture NB distribution for proteins.<br> |      `rate_fore` refers to foreground mean, while `rate_back` refers to background mean. ``scale`` refers to<br> |      foreground mean adjusted for background probability and scaled to reside in simplex.<br> |      ``back_alpha`` and ``back_beta`` are the posterior parameters for ``rate_back``.  ``fore_scale`` is the scaling<br> |      factor that enforces `rate_fore` > `rate_back`.<br> |      <br> |      ``px_[\"r\"]`` and ``py_[\"r\"]`` are the inverse dispersion parameters for genes and protein, respectively.<br> |      <br> |      Parameters<br> |      ----------<br> |      x<br> |          tensor of values with shape ``(batch_size, n_input_genes)``<br> |      y<br> |          tensor of values with shape ``(batch_size, n_input_proteins)``<br> |      batch_index<br> |          array that indicates which batch the cells belong to with shape ``batch_size``<br> |      label<br> |          tensor of cell-types labels with shape (batch_size, n_labels)<br> |      n_samples<br> |          Number of samples to sample from approximate posterior<br> |      cont_covs<br> |          Continuous covariates to condition on<br> |      cat_covs<br> |          Categorical covariates to condition on<br> |  <br> |  loss(self, tensors, inference_outputs, generative_outputs, pro_recons_weight=1.0, kl_weight=1.0) -> Tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]<br> |      Returns the reconstruction loss and the Kullback divergences.<br> |      <br> |      Parameters<br> |      ----------<br> |      x<br> |          tensor of values with shape ``(batch_size, n_input_genes)``<br> |      y<br> |          tensor of values with shape ``(batch_size, n_input_proteins)``<br> |      batch_index<br> |          array that indicates which batch the cells belong to with shape ``batch_size``<br> |      label<br> |          tensor of cell-types labels with shape (batch_size, n_labels)<br> |      <br> |      Returns<br> |      -------<br> |      type<br> |          the reconstruction loss and the Kullback divergences<br> |  <br> |  marginal_ll(self, tensors, n_mc_samples)<br> |      Computes the marginal log likelihood of the data under the model.<br> |  <br> |  sample(self, tensors, n_samples=1)<br> |      Sample from the generative model.<br> |  <br> |  ----------------------------------------------------------------------<br> |  Methods inherited from scvi.module.base._base_module.BaseModuleClass:<br> |  <br> |  forward(self, tensors, get_inference_input_kwargs: 'Optional[dict]' = None, get_generative_input_kwargs: 'Optional[dict]' = None, inference_kwargs: 'Optional[dict]' = None, generative_kwargs: 'Optional[dict]' = None, loss_kwargs: 'Optional[dict]' = None, compute_loss=True) -> 'Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor, torch.Tensor, LossOutput]]'<br> |      Forward pass through the network.<br> |      <br> |      Parameters<br> |      ----------<br> |      tensors<br> |          tensors to pass through<br> |      get_inference_input_kwargs<br> |          Keyword args for ``_get_inference_input()``<br> |      get_generative_input_kwargs<br> |          Keyword args for ``_get_generative_input()``<br> |      inference_kwargs<br> |          Keyword args for ``inference()``<br> |      generative_kwargs<br> |          Keyword args for ``generative()``<br> |      loss_kwargs<br> |          Keyword args for ``loss()``<br> |      compute_loss<br> |          Whether to compute loss on forward pass. This adds<br> |          another return value.<br> |  <br> |  on_load(self, model)<br> |      Callback function run in :meth:`~scvi.model.base.BaseModelClass.load` prior to loading module state dict.<br> |  <br> |  ----------------------------------------------------------------------<br> |  Readonly properties inherited from scvi.module.base._base_module.BaseModuleClass:<br> |  <br> |  device<br> |  <br> |  ----------------------------------------------------------------------<br> |  Data descriptors inherited from scvi.autotune._types.TunableMixin:<br> |  <br> |  __dict__<br> |      dictionary for instance variables (if defined)<br> |  <br> |  __weakref__<br> |      list of weak references to the object (if defined)<br> |  <br> |  ----------------------------------------------------------------------<br> |  Methods inherited from torch.nn.modules.module.Module:<br> |  <br> |  __call__ = _call_impl(self, *input, **kwargs)<br> |  <br> |  __delattr__(self, name)<br> |      Implement delattr(self, name).<br> |  <br> |  __dir__(self)<br> |      Default dir() implementation.<br> |  <br> |  __getattr__(self, name: str) -> Union[torch.Tensor, ForwardRef('Module')]<br> |  <br> |  __repr__(self)<br> |      Return repr(self).<br> |  <br> |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None<br> |      Implement setattr(self, name, value).<br> |  <br> |  __setstate__(self, state)<br> |  <br> |  add_module(self, name: str, module: Union[ForwardRef('Module'), NoneType]) -> None<br> |      Adds a child module to the current module.<br> |      <br> |      The module can be accessed as an attribute using the given name.<br> |      <br> |      Args:<br> |          name (str): name of the child module. The child module can be<br> |              accessed from this module using the given name<br> |          module (Module): child module to be added to the module.<br> |  <br> |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T<br> |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)<br> |      as well as self. Typical use includes initializing the parameters of a model<br> |      (see also :ref:`nn-init-doc`).<br> |      <br> |      Args:<br> |          fn (:class:`Module` -> None): function to be applied to each submodule<br> |      <br> |      Returns:<br> |          Module: self<br> |      <br> |      Example::<br> |      <br> |          >>> @torch.no_grad()<br> |          >>> def init_weights(m):<br> |          >>>     print(m)<br> |          >>>     if type(m) == nn.Linear:<br> |          >>>         m.weight.fill_(1.0)<br> |          >>>         print(m.weight)<br> |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))<br> |          >>> net.apply(init_weights)<br> |          Linear(in_features=2, out_features=2, bias=True)<br> |          Parameter containing:<br> |          tensor([[1., 1.],<br> |                  [1., 1.]], requires_grad=True)<br> |          Linear(in_features=2, out_features=2, bias=True)<br> |          Parameter containing:<br> |          tensor([[1., 1.],<br> |                  [1., 1.]], requires_grad=True)<br> |          Sequential(<br> |            (0): Linear(in_features=2, out_features=2, bias=True)<br> |            (1): Linear(in_features=2, out_features=2, bias=True)<br> |          )<br> |  <br> |  bfloat16(self: ~T) -> ~T<br> |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.<br> |      <br> |      .. note::<br> |          This method modifies the module in-place.<br> |      <br> |      Returns:<br> |          Module: self<br> |  <br> |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]<br> |      Returns an iterator over module buffers.<br> |      <br> |      Args:<br> |          recurse (bool): if True, then yields buffers of this module<br> |              and all submodules. Otherwise, yields only buffers that<br> |              are direct members of this module.<br> |      <br> |      Yields:<br> |          torch.Tensor: module buffer<br> |      <br> |      Example::<br> |      <br> |          >>> # xdoctest: +SKIP(\"undefined vars\")<br> |          >>> for buf in model.buffers():<br> |          >>>     print(type(buf), buf.size())<br> |          <class 'torch.Tensor'> (20L,)<br> |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)<br> |  <br> |  children(self) -> Iterator[ForwardRef('Module')]<br> |      Returns an iterator over immediate children modules.<br> |      <br> |      Yields:<br> |          Module: a child module<br> |  <br> |  cpu(self: ~T) -> ~T<br> |      Moves all model parameters and buffers to the CPU.<br> |      <br> |      .. note::<br> |          This method modifies the module in-place.<br> |      <br> |      Returns:<br> |          Module: self<br> |  <br> |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T<br> |      Moves all model parameters and buffers to the GPU.<br> |      <br> |      This also makes associated parameters and buffers different objects. So<br> |      it should be called before constructing optimizer if the module will<br> |      live on GPU while being optimized.<br> |      <br> |      .. note::<br> |          This method modifies the module in-place.<br> |      <br> |      Args:<br> |          device (int, optional): if specified, all parameters will be<br> |              copied to that device<br> |      <br> |      Returns:<br> |          Module: self<br> |  <br> |  double(self: ~T) -> ~T<br> |      Casts all floating point parameters and buffers to ``double`` datatype.<br> |      <br> |      .. note::<br> |          This method modifies the module in-place.<br> |      <br> |      Returns:<br> |          Module: self<br> |  <br> |  eval(self: ~T) -> ~T<br> |      Sets the module in evaluation mode.<br> |      <br> |      This has any effect only on certain modules. See documentations of<br> |      particular modules for details of their behaviors in training/evaluation<br> |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,<br> |      etc.<br> |      <br> |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.<br> |      <br> |      See :ref:`locally-disable-grad-doc` for a comparison between<br> |      `.eval()` and several similar mechanisms that may be confused with it.<br> |      <br> |      Returns:<br> |          Module: self<br> |  <br> |  extra_repr(self) -> str<br> |      Set the extra representation of the module<br> |      <br> |      To print customized extra information, you should re-implement<br> |      this method in your own modules. Both single-line and multi-line<br> |      strings are acceptable.<br> |  <br> |  float(self: ~T) -> ~T<br> |      Casts all floating point parameters and buffers to ``float`` datatype.<br> |      <br> |      .. note::<br> |          This method modifies the module in-place.<br> |      <br> |      Returns:<br> |          Module: self<br> |  <br> |  get_buffer(self, target: str) -> 'Tensor'<br> |      Returns the buffer given by ``target`` if it exists,<br> |      otherwise throws an error.<br> |      <br> |      See the docstring for ``get_submodule`` for a more detailed<br> |      explanation of this method's functionality as well as how to<br> |      correctly specify ``target``.<br> |      <br> |      Args:<br> |          target: The fully-qualified string name of the buffer<br> |              to look for. (See ``get_submodule`` for how to specify a<br> |              fully-qualified string.)<br> |      <br> |      Returns:<br> |          torch.Tensor: The buffer referenced by ``target``<br> |      <br> |      Raises:<br> |          AttributeError: If the target string references an invalid<br> |              path or resolves to something that is not a<br> |              buffer<br> |  <br> |  get_extra_state(self) -> Any<br> |      Returns any extra state to include in the module's state_dict.<br> |      Implement this and a corresponding :func:`set_extra_state` for your module<br> |      if you need to store extra state. This function is called when building the<br> |      module's `state_dict()`.<br> |      <br> |      Note that extra state should be pickleable to ensure working serialization<br> |      of the state_dict. We only provide provide backwards compatibility guarantees<br> |      for serializing Tensors; other objects may break backwards compatibility if<br> |      their serialized pickled form changes.<br> |      <br> |      Returns:<br> |          object: Any extra state to store in the module's state_dict<br> |  <br> |  get_parameter(self, target: str) -> 'Parameter'<br> |      Returns the parameter given by ``target`` if it exists,<br> |      otherwise throws an error.<br> |      <br> |      See the docstring for ``get_submodule`` for a more detailed<br> |      explanation of this method's functionality as well as how to<br> |      correctly specify ``target``.<br> |      <br> |      Args:<br> |          target: The fully-qualified string name of the Parameter<br> |              to look for. (See ``get_submodule`` for how to specify a<br> |              fully-qualified string.)<br> |      <br> |      Returns:<br> |          torch.nn.Parameter: The Parameter referenced by ``target``<br> |      <br> |      Raises:<br> |          AttributeError: If the target string references an invalid<br> |              path or resolves to something that is not an<br> |              ``nn.Parameter``<br> |  <br> |  get_submodule(self, target: str) -> 'Module'<br> |      Returns the submodule given by ``target`` if it exists,<br> |      otherwise throws an error.<br> |      <br> |      For example, let's say you have an ``nn.Module`` ``A`` that<br> |      looks like this:<br> |      <br> |      .. code-block:: text<br> |      <br> |          A(<br> |              (net_b): Module(<br> |                  (net_c): Module(<br> |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))<br> |                  )<br> |                  (linear): Linear(in_features=100, out_features=200, bias=True)<br> |              )<br> |          )<br> |      <br> |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested<br> |      submodule ``net_b``, which itself has two submodules ``net_c``<br> |      and ``linear``. ``net_c`` then has a submodule ``conv``.)<br> |      <br> |      To check whether or not we have the ``linear`` submodule, we<br> |      would call ``get_submodule(\"net_b.linear\")``. To check whether<br> |      we have the ``conv`` submodule, we would call<br> |      ``get_submodule(\"net_b.net_c.conv\")``.<br> |      <br> |      The runtime of ``get_submodule`` is bounded by the degree<br> |      of module nesting in ``target``. A query against<br> |      ``named_modules`` achieves the same result, but it is O(N) in<br> |      the number of transitive modules. So, for a simple check to see<br> |      if some submodule exists, ``get_submodule`` should always be<br> |      used.<br> |      <br> |      Args:<br> |          target: The fully-qualified string name of the submodule<br> |              to look for. (See above example for how to specify a<br> |              fully-qualified string.)<br> |      <br> |      Returns:<br> |          torch.nn.Module: The submodule referenced by ``target``<br> |      <br> |      Raises:<br> |          AttributeError: If the target string references an invalid<br> |              path or resolves to something that is not an<br> |              ``nn.Module``<br> |  <br> |  half(self: ~T) -> ~T<br> |      Casts all floating point parameters and buffers to ``half`` datatype.<br> |      <br> |      .. note::<br> |          This method modifies the module in-place.<br> |      <br> |      Returns:<br> |          Module: self<br> |  <br> |  ipu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T<br> |      Moves all model parameters and buffers to the IPU.<br> |      <br> |      This also makes associated parameters and buffers different objects. So<br> |      it should be called before constructing optimizer if the module will<br> |      live on IPU while being optimized.<br> |      <br> |      .. note::<br> |          This method modifies the module in-place.<br> |      <br> |      Arguments:<br> |          device (int, optional): if specified, all parameters will be<br> |              copied to that device<br> |      <br> |      Returns:<br> |          Module: self<br> |  <br> |  load_state_dict(self, state_dict: Mapping[str, Any], strict: bool = True)<br> |      Copies parameters and buffers from :attr:`state_dict` into<br> |      this module and its descendants. If :attr:`strict` is ``True``, then<br> |      the keys of :attr:`state_dict` must exactly match the keys returned<br> |      by this module's :meth:`~torch.nn.Module.state_dict` function.<br> |      <br> |      Args:<br> |          state_dict (dict): a dict containing parameters and<br> |              persistent buffers.<br> |          strict (bool, optional): whether to strictly enforce that the keys<br> |              in :attr:`state_dict` match the keys returned by this module's<br> |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``<br> |      <br> |      Returns:<br> |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:<br> |              * **missing_keys** is a list of str containing the missing keys<br> |              * **unexpected_keys** is a list of str containing the unexpected keys<br> |      <br> |      Note:<br> |          If a parameter or buffer is registered as ``None`` and its corresponding key<br> |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a<br> |          ``RuntimeError``.<br> |  <br> |  modules(self) -> Iterator[ForwardRef('Module')]<br> |      Returns an iterator over all modules in the network.<br> |      <br> |      Yields:<br> |          Module: a module in the network<br> |      <br> |      Note:<br> |          Duplicate modules are returned only once. In the following<br> |          example, ``l`` will be returned only once.<br> |      <br> |      Example::<br> |      <br> |          >>> l = nn.Linear(2, 2)<br> |          >>> net = nn.Sequential(l, l)<br> |          >>> for idx, m in enumerate(net.modules()):<br> |          ...     print(idx, '->', m)<br> |      <br> |          0 -> Sequential(<br> |            (0): Linear(in_features=2, out_features=2, bias=True)<br> |            (1): Linear(in_features=2, out_features=2, bias=True)<br> |          )<br> |          1 -> Linear(in_features=2, out_features=2, bias=True)<br> |  <br> |  named_buffers(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.Tensor]]<br> |      Returns an iterator over module buffers, yielding both the<br> |      name of the buffer as well as the buffer itself.<br> |      <br> |      Args:<br> |          prefix (str): prefix to prepend to all buffer names.<br> |          recurse (bool): if True, then yields buffers of this module<br> |              and all submodules. Otherwise, yields only buffers that<br> |              are direct members of this module.<br> |      <br> |      Yields:<br> |          (str, torch.Tensor): Tuple containing the name and buffer<br> |      <br> |      Example::<br> |      <br> |          >>> # xdoctest: +SKIP(\"undefined vars\")<br> |          >>> for name, buf in self.named_buffers():<br> |          >>>    if name in ['running_var']:<br> |          >>>        print(buf.size())<br> |  <br> |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]<br> |      Returns an iterator over immediate children modules, yielding both<br> |      the name of the module as well as the module itself.<br> |      <br> |      Yields:<br> |          (str, Module): Tuple containing a name and child module<br> |      <br> |      Example::<br> |      <br> |          >>> # xdoctest: +SKIP(\"undefined vars\")<br> |          >>> for name, module in model.named_children():<br> |          >>>     if name in ['conv4', 'conv5']:<br> |          >>>         print(module)<br> |  <br> |  named_modules(self, memo: Union[Set[ForwardRef('Module')], NoneType] = None, prefix: str = '', remove_duplicate: bool = True)<br> |      Returns an iterator over all modules in the network, yielding<br> |      both the name of the module as well as the module itself.<br> |      <br> |      Args:<br> |          memo: a memo to store the set of modules already added to the result<br> |          prefix: a prefix that will be added to the name of the module<br> |          remove_duplicate: whether to remove the duplicated module instances in the result<br> |              or not<br> |      <br> |      Yields:<br> |          (str, Module): Tuple of name and module<br> |      <br> |      Note:<br> |          Duplicate modules are returned only once. In the following<br> |          example, ``l`` will be returned only once.<br> |      <br> |      Example::<br> |      <br> |          >>> l = nn.Linear(2, 2)<br> |          >>> net = nn.Sequential(l, l)<br> |          >>> for idx, m in enumerate(net.named_modules()):<br> |          ...     print(idx, '->', m)<br> |      <br> |          0 -> ('', Sequential(<br> |            (0): Linear(in_features=2, out_features=2, bias=True)<br> |            (1): Linear(in_features=2, out_features=2, bias=True)<br> |          ))<br> |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))<br> |  <br> |  named_parameters(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]<br> |      Returns an iterator over module parameters, yielding both the<br> |      name of the parameter as well as the parameter itself.<br> |      <br> |      Args:<br> |          prefix (str): prefix to prepend to all parameter names.<br> |          recurse (bool): if True, then yields parameters of this module<br> |              and all submodules. Otherwise, yields only parameters that<br> |              are direct members of this module.<br> |      <br> |      Yields:<br> |          (str, Parameter): Tuple containing the name and parameter<br> |      <br> |      Example::<br> |      <br> |          >>> # xdoctest: +SKIP(\"undefined vars\")<br> |          >>> for name, param in self.named_parameters():<br> |          >>>    if name in ['bias']:<br> |          >>>        print(param.size())<br> |  <br> |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]<br> |      Returns an iterator over module parameters.<br> |      <br> |      This is typically passed to an optimizer.<br> |      <br> |      Args:<br> |          recurse (bool): if True, then yields parameters of this module<br> |              and all submodules. Otherwise, yields only parameters that<br> |              are direct members of this module.<br> |      <br> |      Yields:<br> |          Parameter: module parameter<br> |      <br> |      Example::<br> |      <br> |          >>> # xdoctest: +SKIP(\"undefined vars\")<br> |          >>> for param in model.parameters():<br> |          >>>     print(type(param), param.size())<br> |          <class 'torch.Tensor'> (20L,)<br> |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)<br> |  <br> |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle<br> |      Registers a backward hook on the module.<br> |      <br> |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and<br> |      the behavior of this function will change in future versions.<br> |      <br> |      Returns:<br> |          :class:`torch.utils.hooks.RemovableHandle`:<br> |              a handle that can be used to remove the added hook by calling<br> |              ``handle.remove()``<br> |  <br> |  register_buffer(self, name: str, tensor: Union[torch.Tensor, NoneType], persistent: bool = True) -> None<br> |      Adds a buffer to the module.<br> |      <br> |      This is typically used to register a buffer that should not to be<br> |      considered a model parameter. For example, BatchNorm's ``running_mean``<br> |      is not a parameter, but is part of the module's state. Buffers, by<br> |      default, are persistent and will be saved alongside parameters. This<br> |      behavior can be changed by setting :attr:`persistent` to ``False``. The<br> |      only difference between a persistent buffer and a non-persistent buffer<br> |      is that the latter will not be a part of this module's<br> |      :attr:`state_dict`.<br> |      <br> |      Buffers can be accessed as attributes using given names.<br> |      <br> |      Args:<br> |          name (str): name of the buffer. The buffer can be accessed<br> |              from this module using the given name<br> |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations<br> |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,<br> |              the buffer is **not** included in the module's :attr:`state_dict`.<br> |          persistent (bool): whether the buffer is part of this module's<br> |              :attr:`state_dict`.<br> |      <br> |      Example::<br> |      <br> |          >>> # xdoctest: +SKIP(\"undefined vars\")<br> |          >>> self.register_buffer('running_mean', torch.zeros(num_features))<br> |  <br> |  register_forward_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle<br> |      Registers a forward hook on the module.<br> |      <br> |      The hook will be called every time after :func:`forward` has computed an output.<br> |      It should have the following signature::<br> |      <br> |          hook(module, input, output) -> None or modified output<br> |      <br> |      The input contains only the positional arguments given to the module.<br> |      Keyword arguments won't be passed to the hooks and only to the ``forward``.<br> |      The hook can modify the output. It can modify the input inplace but<br> |      it will not have effect on forward since this is called after<br> |      :func:`forward` is called.<br> |      <br> |      Returns:<br> |          :class:`torch.utils.hooks.RemovableHandle`:<br> |              a handle that can be used to remove the added hook by calling<br> |              ``handle.remove()``<br> |  <br> |  register_forward_pre_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle<br> |      Registers a forward pre-hook on the module.<br> |      <br> |      The hook will be called every time before :func:`forward` is invoked.<br> |      It should have the following signature::<br> |      <br> |          hook(module, input) -> None or modified input<br> |      <br> |      The input contains only the positional arguments given to the module.<br> |      Keyword arguments won't be passed to the hooks and only to the ``forward``.<br> |      The hook can modify the input. User can either return a tuple or a<br> |      single modified value in the hook. We will wrap the value into a tuple<br> |      if a single value is returned(unless that value is already a tuple).<br> |      <br> |      Returns:<br> |          :class:`torch.utils.hooks.RemovableHandle`:<br> |              a handle that can be used to remove the added hook by calling<br> |              ``handle.remove()``<br> |  <br> |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle<br> |      Registers a backward hook on the module.<br> |      <br> |      The hook will be called every time the gradients with respect to a module<br> |      are computed, i.e. the hook will execute if and only if the gradients with<br> |      respect to module outputs are computed. The hook should have the following<br> |      signature::<br> |      <br> |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None<br> |      <br> |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients<br> |      with respect to the inputs and outputs respectively. The hook should<br> |      not modify its arguments, but it can optionally return a new gradient with<br> |      respect to the input that will be used in place of :attr:`grad_input` in<br> |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given<br> |      as positional arguments and all kwarg arguments are ignored. Entries<br> |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor<br> |      arguments.<br> |      <br> |      For technical reasons, when this hook is applied to a Module, its forward function will<br> |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view<br> |      of each Tensor returned by the Module's forward function.<br> |      <br> |      .. warning ::<br> |          Modifying inputs or outputs inplace is not allowed when using backward hooks and<br> |          will raise an error.<br> |      <br> |      Returns:<br> |          :class:`torch.utils.hooks.RemovableHandle`:<br> |              a handle that can be used to remove the added hook by calling<br> |              ``handle.remove()``<br> |  <br> |  register_load_state_dict_post_hook(self, hook)<br> |      Registers a post hook to be run after module's ``load_state_dict``<br> |      is called.<br> |      <br> |      It should have the following signature::<br> |          hook(module, incompatible_keys) -> None<br> |      <br> |      The ``module`` argument is the current module that this hook is registered<br> |      on, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting<br> |      of attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``<br> |      is a ``list`` of ``str`` containing the missing keys and<br> |      ``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.<br> |      <br> |      The given incompatible_keys can be modified inplace if needed.<br> |      <br> |      Note that the checks performed when calling :func:`load_state_dict` with<br> |      ``strict=True`` are affected by modifications the hook makes to<br> |      ``missing_keys`` or ``unexpected_keys``, as expected. Additions to either<br> |      set of keys will result in an error being thrown when ``strict=True``, and<br> |      clearning out both missing and unexpected keys will avoid an error.<br> |      <br> |      Returns:<br> |          :class:`torch.utils.hooks.RemovableHandle`:<br> |              a handle that can be used to remove the added hook by calling<br> |              ``handle.remove()``<br> |  <br> |  register_module(self, name: str, module: Union[ForwardRef('Module'), NoneType]) -> None<br> |      Alias for :func:`add_module`.<br> |  <br> |  register_parameter(self, name: str, param: Union[torch.nn.parameter.Parameter, NoneType]) -> None<br> |      Adds a parameter to the module.<br> |      <br> |      The parameter can be accessed as an attribute using given name.<br> |      <br> |      Args:<br> |          name (str): name of the parameter. The parameter can be accessed<br> |              from this module using the given name<br> |          param (Parameter or None): parameter to be added to the module. If<br> |              ``None``, then operations that run on parameters, such as :attr:`cuda`,<br> |              are ignored. If ``None``, the parameter is **not** included in the<br> |              module's :attr:`state_dict`.<br> |  <br> |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T<br> |      Change if autograd should record operations on parameters in this<br> |      module.<br> |      <br> |      This method sets the parameters' :attr:`requires_grad` attributes<br> |      in-place.<br> |      <br> |      This method is helpful for freezing part of the module for finetuning<br> |      or training parts of a model individually (e.g., GAN training).<br> |      <br> |      See :ref:`locally-disable-grad-doc` for a comparison between<br> |      `.requires_grad_()` and several similar mechanisms that may be confused with it.<br> |      <br> |      Args:<br> |          requires_grad (bool): whether autograd should record operations on<br> |                                parameters in this module. Default: ``True``.<br> |      <br> |      Returns:<br> |          Module: self<br> |  <br> |  set_extra_state(self, state: Any)<br> |      This function is called from :func:`load_state_dict` to handle any extra state<br> |      found within the `state_dict`. Implement this function and a corresponding<br> |      :func:`get_extra_state` for your module if you need to store extra state within its<br> |      `state_dict`.<br> |      <br> |      Args:<br> |          state (dict): Extra state from the `state_dict`<br> |  <br> |  share_memory(self: ~T) -> ~T<br> |      See :meth:`torch.Tensor.share_memory_`<br> |  <br> |  state_dict(self, *args, destination=None, prefix='', keep_vars=False)<br> |      Returns a dictionary containing references to the whole state of the module.<br> |      <br> |      Both parameters and persistent buffers (e.g. running averages) are<br> |      included. Keys are corresponding parameter and buffer names.<br> |      Parameters and buffers set to ``None`` are not included.<br> |      <br> |      .. note::<br> |          The returned object is a shallow copy. It contains references<br> |          to the module's parameters and buffers.<br> |      <br> |      .. warning::<br> |          Currently ``state_dict()`` also accepts positional arguments for<br> |          ``destination``, ``prefix`` and ``keep_vars`` in order. However,<br> |          this is being deprecated and keyword arguments will be enforced in<br> |          future releases.<br> |      <br> |      .. warning::<br> |          Please avoid the use of argument ``destination`` as it is not<br> |          designed for end-users.<br> |      <br> |      Args:<br> |          destination (dict, optional): If provided, the state of module will<br> |              be updated into the dict and the same object is returned.<br> |              Otherwise, an ``OrderedDict`` will be created and returned.<br> |              Default: ``None``.<br> |          prefix (str, optional): a prefix added to parameter and buffer<br> |              names to compose the keys in state_dict. Default: ``''``.<br> |          keep_vars (bool, optional): by default the :class:`~torch.Tensor` s<br> |              returned in the state dict are detached from autograd. If it's<br> |              set to ``True``, detaching will not be performed.<br> |              Default: ``False``.<br> |      <br> |      Returns:<br> |          dict:<br> |              a dictionary containing a whole state of the module<br> |      <br> |      Example::<br> |      <br> |          >>> # xdoctest: +SKIP(\"undefined vars\")<br> |          >>> module.state_dict().keys()<br> |          ['bias', 'weight']<br> |  <br> |  to(self, *args, **kwargs)<br> |      Moves and/or casts the parameters and buffers.<br> |      <br> |      This can be called as<br> |      <br> |      .. function:: to(device=None, dtype=None, non_blocking=False)<br> |         :noindex:<br> |      <br> |      .. function:: to(dtype, non_blocking=False)<br> |         :noindex:<br> |      <br> |      .. function:: to(tensor, non_blocking=False)<br> |         :noindex:<br> |      <br> |      .. function:: to(memory_format=torch.channels_last)<br> |         :noindex:<br> |      <br> |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts<br> |      floating point or complex :attr:`dtype`\\ s. In addition, this method will<br> |      only cast the floating point or complex parameters and buffers to :attr:`dtype`<br> |      (if given). The integral parameters and buffers will be moved<br> |      :attr:`device`, if that is given, but with dtypes unchanged. When<br> |      :attr:`non_blocking` is set, it tries to convert/move asynchronously<br> |      with respect to the host if possible, e.g., moving CPU Tensors with<br> |      pinned memory to CUDA devices.<br> |      <br> |      See below for examples.<br> |      <br> |      .. note::<br> |          This method modifies the module in-place.<br> |      <br> |      Args:<br> |          device (:class:`torch.device`): the desired device of the parameters<br> |              and buffers in this module<br> |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of<br> |              the parameters and buffers in this module<br> |          tensor (torch.Tensor): Tensor whose dtype and device are the desired<br> |              dtype and device for all parameters and buffers in this module<br> |          memory_format (:class:`torch.memory_format`): the desired memory<br> |              format for 4D parameters and buffers in this module (keyword<br> |              only argument)<br> |      <br> |      Returns:<br> |          Module: self<br> |      <br> |      Examples::<br> |      <br> |          >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")<br> |          >>> linear = nn.Linear(2, 2)<br> |          >>> linear.weight<br> |          Parameter containing:<br> |          tensor([[ 0.1913, -0.3420],<br> |                  [-0.5113, -0.2325]])<br> |          >>> linear.to(torch.double)<br> |          Linear(in_features=2, out_features=2, bias=True)<br> |          >>> linear.weight<br> |          Parameter containing:<br> |          tensor([[ 0.1913, -0.3420],<br> |                  [-0.5113, -0.2325]], dtype=torch.float64)<br> |          >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA1)<br> |          >>> gpu1 = torch.device(\"cuda:1\")<br> |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)<br> |          Linear(in_features=2, out_features=2, bias=True)<br> |          >>> linear.weight<br> |          Parameter containing:<br> |          tensor([[ 0.1914, -0.3420],<br> |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')<br> |          >>> cpu = torch.device(\"cpu\")<br> |          >>> linear.to(cpu)<br> |          Linear(in_features=2, out_features=2, bias=True)<br> |          >>> linear.weight<br> |          Parameter containing:<br> |          tensor([[ 0.1914, -0.3420],<br> |                  [-0.5112, -0.2324]], dtype=torch.float16)<br> |      <br> |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)<br> |          >>> linear.weight<br> |          Parameter containing:<br> |          tensor([[ 0.3741+0.j,  0.2382+0.j],<br> |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)<br> |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))<br> |          tensor([[0.6122+0.j, 0.1150+0.j],<br> |                  [0.6122+0.j, 0.1150+0.j],<br> |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)<br> |  <br> |  to_empty(self: ~T, *, device: Union[str, torch.device]) -> ~T<br> |      Moves the parameters and buffers to the specified device without copying storage.<br> |      <br> |      Args:<br> |          device (:class:`torch.device`): The desired device of the parameters<br> |              and buffers in this module.<br> |      <br> |      Returns:<br> |          Module: self<br> |  <br> |  train(self: ~T, mode: bool = True) -> ~T<br> |      Sets the module in training mode.<br> |      <br> |      This has any effect only on certain modules. See documentations of<br> |      particular modules for details of their behaviors in training/evaluation<br> |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,<br> |      etc.<br> |      <br> |      Args:<br> |          mode (bool): whether to set training mode (``True``) or evaluation<br> |                       mode (``False``). Default: ``True``.<br> |      <br> |      Returns:<br> |          Module: self<br> |  <br> |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T<br> |      Casts all parameters and buffers to :attr:`dst_type`.<br> |      <br> |      .. note::<br> |          This method modifies the module in-place.<br> |      <br> |      Args:<br> |          dst_type (type or string): the desired type<br> |      <br> |      Returns:<br> |          Module: self<br> |  <br> |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T<br> |      Moves all model parameters and buffers to the XPU.<br> |      <br> |      This also makes associated parameters and buffers different objects. So<br> |      it should be called before constructing optimizer if the module will<br> |      live on XPU while being optimized.<br> |      <br> |      .. note::<br> |          This method modifies the module in-place.<br> |      <br> |      Arguments:<br> |          device (int, optional): if specified, all parameters will be<br> |              copied to that device<br> |      <br> |      Returns:<br> |          Module: self<br> |  <br> |  zero_grad(self, set_to_none: bool = False) -> None<br> |      Sets gradients of all model parameters to zero. See similar function<br> |      under :class:`torch.optim.Optimizer` for more context.<br> |      <br> |      Args:<br> |          set_to_none (bool): instead of setting to zero, set the grads to None.<br> |              See :meth:`torch.optim.Optimizer.zero_grad` for details.<br> |  <br> |  ----------------------------------------------------------------------<br> |  Data and other attributes inherited from torch.nn.modules.module.Module:<br> |  <br> |  T_destination = ~T_destination<br> |  <br> |  __annotations__ = {'__call__': typing.Callable[..., typing.Any], '_bac...<br> |  <br> |  dump_patches = False<br><br><br>",
    "topmodules": {
      "html": "<div id=\"topmods\" class=\"form-group shiny-input-radiogroup shiny-input-container\" role=\"radiogroup\" aria-labelledby=\"topmods-label\">\n  <label class=\"control-label\" id=\"topmods-label\" for=\"topmods\">modules<\/label>\n  <div class=\"shiny-options-group\">\n    <div class=\"radio\">\n      <label>\n        <input type=\"radio\" name=\"topmods\" value=\"autotune\" checked=\"checked\"/>\n        <span>autotune<\/span>\n      <\/label>\n    <\/div>\n    <div class=\"radio\">\n      <label>\n        <input type=\"radio\" name=\"topmods\" value=\"data\"/>\n        <span>data<\/span>\n      <\/label>\n    <\/div>\n    <div class=\"radio\">\n      <label>\n        <input type=\"radio\" name=\"topmods\" value=\"dataloaders\"/>\n        <span>dataloaders<\/span>\n      <\/label>\n    <\/div>\n    <div class=\"radio\">\n      <label>\n        <input type=\"radio\" name=\"topmods\" value=\"distributions\"/>\n        <span>distributions<\/span>\n      <\/label>\n    <\/div>\n    <div class=\"radio\">\n      <label>\n        <input type=\"radio\" name=\"topmods\" value=\"external\"/>\n        <span>external<\/span>\n      <\/label>\n    <\/div>\n    <div class=\"radio\">\n      <label>\n        <input type=\"radio\" name=\"topmods\" value=\"logging\"/>\n        <span>logging<\/span>\n      <\/label>\n    <\/div>\n    <div class=\"radio\">\n      <label>\n        <input type=\"radio\" name=\"topmods\" value=\"model\"/>\n        <span>model<\/span>\n      <\/label>\n    <\/div>\n    <div class=\"radio\">\n      <label>\n        <input type=\"radio\" name=\"topmods\" value=\"module\"/>\n        <span>module<\/span>\n      <\/label>\n    <\/div>\n    <div class=\"radio\">\n      <label>\n        <input type=\"radio\" name=\"topmods\" value=\"nn\"/>\n        <span>nn<\/span>\n      <\/label>\n    <\/div>\n    <div class=\"radio\">\n      <label>\n        <input type=\"radio\" name=\"topmods\" value=\"train\"/>\n        <span>train<\/span>\n      <\/label>\n    <\/div>\n    <div class=\"radio\">\n      <label>\n        <input type=\"radio\" name=\"topmods\" value=\"utils\"/>\n        <span>utils<\/span>\n      <\/label>\n    <\/div>\n  <\/div>\n<\/div>",
      "deps": [

      ]
    }
  },
  "export": {

  }
}
